import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping

# Charger les données depuis le fichier CSV
csv_file_path = r"C:\Ecole\4eme annee\machine learning\LVMH - Donnees Historiques (1).csv"
data = pd.read_csv(csv_file_path, delimiter=";", encoding="utf-8")
data.columns = data.columns.str.strip()

# Convertir la colonne 'Date' au bon format
data['date'] = pd.to_datetime(data['date'], format="%d/%m/%Y")

# Nettoyer la colonne "Variation %"
data['Variation %'] = data['Variation %'].astype(str)
data['Variation %'] = data['Variation %'].str.replace("%", "").str.replace(",", ".")
data['Variation %'] = data['Variation %'].replace("", "0").astype(float) / 100

# Nettoyer la colonne 'Vol.'
def convert_volume(value):
    value = str(value)
    if 'K' in value:
        return float(value.replace('K', '').replace(',', '.')) * 1000
    elif 'M' in value:
        return float(value.replace('M', '').replace(',', '.')) * 1_000_000
    else:
        return float(value.replace(',', '.'))

data['Vol.'] = data['Vol.'].apply(convert_volume)

# Réindexation des données
data = data.reset_index(drop=True)

# Normaliser les données pour LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_returns = scaler.fit_transform(data['Variation %'].values.reshape(-1, 1))

# Préparer les données avec une fenêtre de lookback
def preprocess_data(data, lookback=90):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i])
        y.append(data[i])
    return np.array(X), np.array(y)

lookback = 90
X, y = preprocess_data(scaled_returns, lookback)

# Diviser les ensembles d'entraînement et de test
split_index = int(len(X) * 0.8)
X_train_lstm, X_test_lstm = X[:split_index], X[split_index:]
y_train_lstm, y_test_lstm = y[:split_index], y[split_index:]

# Reshaper pour LSTM
X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))
X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))

# Construire le modèle LSTM
def build_lstm(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(100, activation='relu', return_sequences=True, input_shape=input_shape),
        tf.keras.layers.LSTM(100, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Construire et entraîner le modèle
lstm_model = build_lstm((lookback, 1))
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

lstm_model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping])

# Prédictions
lstm_predictions = lstm_model.predict(X_test_lstm)

# Désnormaliser les prédictions
lstm_predictions = scaler.inverse_transform(lstm_predictions)
y_test_lstm = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))

# Évaluation du modèle
def evaluate_model(y_true, y_pred, dates):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"MSE: {mse:.4f}, R2: {r2:.4f}")

    # Plot des résultats
    plt.figure(figsize=(10, 6))
    plt.plot(dates, y_true, label='Actual')
    plt.plot(dates, y_pred, label='Predicted', linestyle='--')
    plt.legend()
    plt.title('Actual vs Predicted Returns')
    plt.xlabel('Date')
    plt.ylabel('Returns')
    plt.show()

evaluate_model(y_test_lstm, lstm_predictions, data['date'][-len(y_test_lstm):])

# Calcul de la VaR
def calculate_var(predictions, confidence_level=0.95):
    if predictions is None or len(predictions) == 0:
        print("Erreur : les predictions sont vides ou non definies.")
        return None
    if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):
        print("Erreur : les predictions contiennent des valeurs NaN ou infinies.")
        return None
    try:
        var_threshold = np.percentile(predictions, (1 - confidence_level) * 100)
        print(f"Calculated VaR Threshold: {var_threshold:.4f}")
        return var_threshold
    except Exception as e:
        print(f"Erreur lors du calcul de la VaR : {e}")
        return None

var_threshold = calculate_var(lstm_predictions)

if var_threshold is not None:
    def backtest_var(y_true, var, confidence_level=0.95):
        if y_true is None or len(y_true) == 0:
            print("Erreur : les donnees reelles sont vides ou non definies.")
            return
        y_true = np.squeeze(y_true)
        if np.any(np.isnan(y_true)) or np.any(np.isinf(y_true)):
            print("Erreur : y_true contient des valeurs NaN ou infinies.")
            return
        
        violations = np.sum(y_true < var)
        expected_violations = len(y_true) * (1 - confidence_level)
        print(f"Observed Violations: {violations}, Expected Violations: {expected_violations:.2f}")
        if violations > 0:
            print("Violations detected. VaR exceeded.")
        else:
            print("No violations detected. VaR threshold not exceeded.")

    backtest_var(y_test_lstm, var_threshold)
else:
    print("Erreur : La VaR n'a pas ete calcuee. Impossible de continuer le backtest.")
