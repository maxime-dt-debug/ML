import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import norm
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from sklearn.base import BaseEstimator, RegressorMixin

# Charger les données depuis le fichier CSV
csv_file_path = r"C:\Ecole\4eme annee\machine learning\LVMH - Donnees Historiques (1).csv"
data = pd.read_csv(csv_file_path, delimiter=";", encoding="utf-8")
data.columns = data.columns.str.strip()

# Convertir la colonne 'Date' au bon format
data['date'] = pd.to_datetime(data['date'], format="%d/%m/%Y")

# Nettoyer la colonne "Variation %"
data['Variation %'] = data['Variation %'].astype(str)
data['Variation %'] = data['Variation %'].str.replace("%", "").str.replace(",", ".")
data['Variation %'] = data['Variation %'].replace("", "0").astype(float) / 100

# Nettoyer la colonne 'Vol.'
def convert_volume(value):
    value = str(value)
    if 'K' in value:
        return float(value.replace('K', '').replace(',', '.')) * 1000
    elif 'M' in value:
        return float(value.replace('M', '').replace(',', '.')) * 1_000_000
    else:
        return float(value.replace(',', '.'))

data['Vol.'] = data['Vol.'].apply(convert_volume)

# Réindexation des données
data = data.reset_index(drop=True)

# Normaliser les données pour LSTM
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_returns = scaler.fit_transform(data['Variation %'].values.reshape(-1, 1))

# Lissage des rendements pour réduire l'oscillation
def smooth_returns(returns, window=5):
    # Retourne un tableau unidimensionnel après le lissage
    return pd.Series(returns).rolling(window=window, min_periods=1).mean().to_numpy()

# Lisser les rendements
smoothed_returns = smooth_returns(scaled_returns.flatten(), window=5)

# Préparer les données avec une fenêtre de lookback
def preprocess_data(data, lookback=90):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i])
        y.append(data[i])
    return np.array(X), np.array(y)

lookback = 90
X, y = preprocess_data(smoothed_returns, lookback)

# Diviser les ensembles d'entraînement et de test
split_index = int(len(X) * 0.8)
X_train_lstm, X_test_lstm = X[:split_index], X[split_index:]
y_train_lstm, y_test_lstm = y[:split_index], y[split_index:]

# Reshaper pour LSTM
X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))
X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))

# Classe LSTMModel pour intégrer sans GridSearchCV
class LSTMModel(BaseEstimator, RegressorMixin):
    def __init__(self, units=100, dropout_rate=0.3, batch_size=64, epochs=2):  # 2 epochs pour l'entraînement
        self.units = units
        self.dropout_rate = dropout_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.model = None

    def build_model(self, input_shape):
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(self.units, activation='relu', return_sequences=True, input_shape=input_shape),
            tf.keras.layers.LSTM(self.units, activation='relu'),
            tf.keras.layers.Dropout(self.dropout_rate),
            tf.keras.layers.Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        return model

    def fit(self, X, y):
        # Construire le modèle
        self.model = self.build_model((X.shape[1], 1))
        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2, callbacks=[early_stopping])
        return self

    def predict(self, X):
        return self.model.predict(X)

# Fixer les paramètres (epochs à 2, 2 couches LSTM)
best_params = {
    'units': 150,  # Augmenter le nombre d'unités dans chaque couche LSTM
    'dropout_rate': 0.3,  # Taux de dropout à 0.3
    'batch_size': 64,
    'epochs': 2  # Fixé à 2 époques
}

# Construire et entraîner le modèle avec les meilleurs paramètres
best_lstm_model = LSTMModel(units=best_params['units'], dropout_rate=best_params['dropout_rate'],
                             batch_size=best_params['batch_size'], epochs=best_params['epochs'])

best_lstm_model.fit(X_train_lstm, y_train_lstm)

# Prédictions
best_lstm_predictions = best_lstm_model.predict(X_test_lstm)

# Désnormaliser les prédictions
best_lstm_predictions = scaler.inverse_transform(best_lstm_predictions)
y_test_lstm = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))

# Evaluation du modèle
def evaluate_model(y_true, y_pred, dates):
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}")

    # Plot des résultats
    plt.figure(figsize=(10, 6))
    plt.plot(dates, y_true, label='Actual')
    plt.plot(dates, y_pred, label='Predicted', linestyle='--')
    plt.legend()
    plt.title('Actual vs Predicted Returns')
    plt.xlabel('Date')
    plt.ylabel('Returns')
    plt.show()

evaluate_model(y_test_lstm, best_lstm_predictions, data['date'][-len(y_test_lstm):])


# Fonction pour calculer la VaR Historique
def calculate_var_historical(returns, confidence_level=0.95):
    print("Calcul de la VaR Historique...")
    var_threshold = np.percentile(returns, (1 - confidence_level) * 100)
    print(f"VaR Historique calculee : {var_threshold}")
    return var_threshold

# Fonction pour calculer la VaR Parametrique
def calculate_var_parametric(returns, confidence_level=0.95):
    print("Calcul de la VaR Parametrique...")
    mean = np.mean(returns)
    std_dev = np.std(returns)
    z_score = norm.ppf(1 - confidence_level)
    var_threshold = mean + z_score * std_dev
    print(f"VaR Parametrique calculee : {var_threshold}")
    return var_threshold

# Fonction de backtest pour la VaR
def backtest_var(returns, var, confidence_level=0.95, var_type="Historique"):
    print(f"\nBacktest de la VaR ({var_type})...")
    violations = np.sum(returns < var)
    expected_violations = len(returns) * (1 - confidence_level)
    print(f"Observed Violations: {violations}, Expected Violations: {expected_violations:.2f}")
    print(f"Proportion of Violations: {violations / len(returns):.4f}")
    
    # Calculer le score de 1 à 10 selon les violations
    violation_ratio = violations / len(returns)
    score = max(0, 10 - (violation_ratio * 100))  # Donne un score de 10 pour peu de violations, et diminue avec les violations.
    print(f"Score de la VaR (sur 10): {score:.2f}")
    
    return violations, expected_violations, score

# Calculer les VaR
returns_actual = best_lstm_predictions.flatten()  # Utilisez les rendements réels calculés à partir des prédictions
confidence_level = 0.95
var_historical = calculate_var_historical(returns_actual, confidence_level)
var_parametric = calculate_var_parametric(returns_actual, confidence_level)

# Backtester les VaR
violations_hist, expected_violations_hist, score_hist = backtest_var(returns_actual, var_historical, confidence_level, var_type="Historique")
violations_param, expected_violations_param, score_param = backtest_var(returns_actual, var_parametric, confidence_level, var_type="Parametrique")

# Affichage des résultats
print("\n--- Resultats finaux ---")
print(f"VaR Historique ({confidence_level*100}%): {var_historical:.4f}")
print(f"VaR Parametrique ({confidence_level*100}%): {var_parametric:.4f}")
print(f"Violations Historiques: {violations_hist}, Expected: {expected_violations_hist:.2f}")
print(f"Violations Parametriques: {violations_param}, Expected: {expected_violations_param:.2f}")
print(f"Score VaR Historique: {score_hist:.2f}")
print(f"Score VaR Parametrique: {score_param:.2f}")

# Visualisation des rendements et de la VaR
plt.figure(figsize=(12, 6))
plt.plot(returns_actual, label='Returns', color='blue', alpha=0.7)
plt.axhline(var_historical, color='red', linestyle='--', label='VaR Historique')
plt.axhline(var_parametric, color='orange', linestyle='--', label='VaR Parametrique')
plt.title('Rendements et VaR')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Returns')
plt.show()
